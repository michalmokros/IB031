{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Tutorial 05 â€“ Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is a task of identifying clusters of similar data points in the dataset based on distances in feature space and assigning them cluster label. It is very often computed on unlabeled data and so it is an example of unsupervised technique. Because of unknown labels there is no single correct answer. In this tutorial we will cover 4 different clustering techniques, external and internal evaluation metrics and some other techniques. For more technical information and other algorithms see [scikit-learn documentation on clustering](https://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Agglomerative Clustering (HAC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea of HAC is to build clusters bottom up by unifying closest clusters into a single larger one. This process creates a hierarchy of clusters (dendrogram) and final clustering is obtained by making a 'cut' in the dendrogram.\n",
    "\n",
    "Let's generate some dummy data to test agglomerative clustering. We will make three fairly well divided clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()  # make plots nicer\n",
    "\n",
    "np.random.seed(42)  # set seed for reproducibility\n",
    "\n",
    "data = pd.DataFrame(columns=[\"x\", \"y\", \"label\"])  # prepare dataframe for datapoints\n",
    "\n",
    "# generate three clusters\n",
    "for i, mean_x in enumerate((0.25, 0.5, 0.75)):\n",
    "    # generate some multivariate normal data\n",
    "    x = np.random.normal(mean_x, 0.03, 100)\n",
    "    y = np.random.normal(500, 100, 100)\n",
    "    data = data.append(pd.DataFrame({\"x\": x, \"y\": y, \"label\": i}))\n",
    "\n",
    "\n",
    "def plot_clusters(data, clusters):\n",
    "    \"\"\"\n",
    "    Plots clusters using scatter plot and color them acordingly.\n",
    "    \n",
    "    :param pd.DataFrame data: dataframe with datapoints havig columns \"x\" and \"y\"\n",
    "    :param list of int clusters: cluster label for each of the datapoint\n",
    "    \"\"\"\n",
    "    # find outliers not belonging to any cluster (only relevant for DBSCAN)\n",
    "    outlier_indices = clusters == -1\n",
    "\n",
    "    # plot points and color them by cluster/label\n",
    "    ax = sns.scatterplot(\n",
    "        data=data[~outlier_indices],\n",
    "        x=\"x\",\n",
    "        y=\"y\",\n",
    "        hue=clusters[~outlier_indices],\n",
    "        palette=\"muted\",\n",
    "        legend=False,\n",
    "    )\n",
    "\n",
    "    if outlier_indices.any():\n",
    "        ax = sns.scatterplot(\n",
    "            data=data[outlier_indices], x=\"x\", y=\"y\", color=\"black\", ax=ax\n",
    "        )\n",
    "    return ax\n",
    "\n",
    "\n",
    "# examine the data\n",
    "plot_clusters(data, data.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train a model and cluster the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "data_train = data.drop(columns=[\"label\"])  # remove cluster label\n",
    "\n",
    "ac = AgglomerativeClustering(n_clusters=3)  # try to find three clusters in the data\n",
    "clusters = ac.fit_predict(data_train)  # save predicted clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspected clusters found by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(data, clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That clustering is really bad. All three clusters are mixed together. Its because we forgot to normalize features. Distances between points in y-axis are much larger than in x-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>Exercise 1</b></div>\n",
    "\n",
    "Make a pipeline that will scale both features into range [0, 1] and then run agglomerative clustering. If you did everything right, you should see the same clusters (except colors might be shuffled) as in the first figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# scale both axis to interval [0, 1]\n",
    "pipeline = make_pipeline(MinMaxScaler(), AgglomerativeClustering(n_clusters=3))\n",
    "better_clusters = pipeline.fit_predict(data_train)\n",
    "plot_clusters(data, better_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This highlights importance of preprocessing before running clustering. It is also necessary to decide if and what scaling to use. Having features unscaled is equivalent to having weighted distance metric where features with higher larger absolute values (in our case y values) are more influential.\n",
    "\n",
    "We can visualize the hierarchy of clusters (dendrogram). The height of the vertical lines is equivalent to 'distance' between two clusters being merged. If we cut at around height of 3, we get three clusters represented by tree different colors. These are exactly the clusters we see on figure above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Z = linkage(pipeline[-2].fit_transform(data_train), \"ward\")\n",
    "fig = plt.figure(figsize=(25, 10))\n",
    "dn = dendrogram(Z, distance_sort=True, color_threshold=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's explore some real-life dataset and see what we can cluster. Let's load dataset about world flags. For each country, we have its basic information and then a lot of characterization of how the flag looks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = pd.read_csv(\"https://www.fi.muni.cz/~xcechak1/IB031/datasets/flags.csv\")\n",
    "flags.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we need to convert categorical columns to category dtype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns = flags.select_dtypes(include=\"object\")\n",
    "flags[object_columns.columns] = object_columns.astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do the preprocessing and clustering. We will one-hot encode categorical values, standardize numeric values, and finally scale everything into range [0, 1]. We will use all features except country name and code as both are unique to each entry and landmass column will serve as true label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_selector, make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    make_column_transformer(\n",
    "        (OneHotEncoder(), make_column_selector(dtype_include=\"category\")),\n",
    "        (StandardScaler(), [\"area\", \"population\"]),\n",
    "        remainder=\"passthrough\",\n",
    "    ),\n",
    "    MinMaxScaler(),\n",
    "    AgglomerativeClustering(n_clusters=6),\n",
    ")\n",
    "\n",
    "flags_train = flags.drop(columns=[\"name\", \"country_code\", \"landmass\"])\n",
    "landmass = flags.landmass\n",
    "flags_clusters = pipeline.fit_predict(flags_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would like to gain insight into what clusters were actually found. We cannot plot the data in a straight forward scatter plot as they have many dimensions. Luckily, we can reduce the dimensionality using Principal Component Analysis (PCA) we saw in tutorial 01. PCA will project the data into given number of dimensions (2 in our case) and retain as much variation in the data as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_pipeline = make_pipeline(pipeline[:-1], PCA(n_components=2),)\n",
    "flags_pca = pca_pipeline.fit_transform(flags_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags_pca = pd.DataFrame(flags_pca, columns=[\"x\", \"y\"])\n",
    "plot_clusters(flags_pca, flags_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is still a but abstract, let's add country flags. First download the archive [flags.zip](https://www.fi.muni.cz/~xcechak1/IB031/assets/flags.zip) with country flags and extract the folder `flags` into the directory with this notebook. Then you should see scatter plot with flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
    "\n",
    "\n",
    "def get_image(country_code):\n",
    "    path = f\"flags/{country_code.lower()}.png\"\n",
    "    return OffsetImage(plt.imread(path), zoom=1)\n",
    "\n",
    "\n",
    "ax = plot_clusters(flags_pca, flags_clusters)\n",
    "country_codes = flags.country_code.str.lower()\n",
    "for x0, y0, country_code in zip(flags_pca.x, flags_pca.y, country_codes):\n",
    "    ab = AnnotationBbox(get_image(country_code), (x0, y0), frameon=False)\n",
    "    ax.add_artist(ab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The projection did put similar looking flags closer to each other; mostly blue flags on the right, mostly red on the bottom, and mostly green in the top left corner. But the clusters were scatter around the plot, so let's see if how much they are consistent with `landmass`.\n",
    "\n",
    "There are external metrics for evaluation of clustering with known labels. We will use Adjusted Rand index, Adjusted Mutual Information, and V-measure. All three measures are measuring 'agreement' between the two labelings. You can read more on these measure and their mathematical formulation in [scikit-learn documentation](https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>Exercise 2</b></div>\n",
    "\n",
    "Evaluate clusters found in flags using Adjusted Rand Index, Adjusted Mutual Information and V-measure. Expected values of these scores are:\n",
    "* ARI: 0.168\n",
    "* AMI: 0.202\n",
    "* V-measure: 0.236"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    adjusted_rand_score,\n",
    "    adjusted_mutual_info_score,\n",
    "    v_measure_score,\n",
    ")\n",
    "\n",
    "print(\"ARI:\", adjusted_rand_score(landmass, flags_clusters))\n",
    "print(\"AMI:\", adjusted_mutual_info_score(landmass, flags_clusters))\n",
    "print(\"V-measure:\", v_measure_score(landmass, flags_clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K means\n",
    "\n",
    "K means algorithm relies on iterative optimization of cluster centroids. Initial centroids are selected randomly a therefore it usually needs multiple restarts or sophisticated centroid initialization to find something closer to global optimum. On the other hand, it is quite fast and simple.\n",
    "\n",
    "Let's load another synthetic dataset and try out k means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation = pd.read_csv(\n",
    "    \"https://www.fi.muni.cz/~xcechak1/IB031/datasets/aggregation.csv\"\n",
    ")\n",
    "plot_clusters(aggregation, aggregation.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the example of randomness of k means algorithm. The default implementation in `scikit-learn` will restart the algorithm 10 times and select clusters from the best performing run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "aggregation_train = aggregation.drop(columns=\"label\")\n",
    "\n",
    "# three runs with different random seed\n",
    "for i in range(3):\n",
    "    # set random state and only one run\n",
    "    kmeans = KMeans(n_clusters=7, random_state=i, n_init=1)\n",
    "    clusters = kmeans.fit_predict(aggregation_train)\n",
    "\n",
    "    plot_clusters(aggregation, clusters)\n",
    "    # plot centroids\n",
    "    sns.scatterplot(\n",
    "        x=kmeans.cluster_centers_[:, 0],\n",
    "        y=kmeans.cluster_centers_[:, 1],\n",
    "        marker=\"X\",\n",
    "        color=\"r\",\n",
    "        s=200,\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Red crosses indicate final centroids of the clusters. Different random states (and by extension centroid initializations) result in different clusters.\n",
    "\n",
    "Now the algorithm needs to select the best clustering of different runs. It does not, however, have any ground truth labels for comparison. This is also the case in most clustering applications where we have data but no clear definition of clusters. We would like the clustering algorithm to give us some condense information of what is in the date, what clusters data points are present in the data.\n",
    "\n",
    "To evaluate the clustering without any labels we need so called internal evaluation criteria. These are often based on judging cluster compactness (small within cluster distances) and separation (large between cluster distances). Examples of these measures are Silhouette Coefficient, Calinski-Harabasz index, and Davies-Bouldin index.\n",
    "\n",
    "Since we do not have any ground truth labels we also do not know how many clusters should we be looking for. However, algorithms like K-means need to know the number of clusters in advance. Typical approach to navigate around this problem is to try different values of $k$ and then select the best performing one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>Exercise 3</b></div>\n",
    "\n",
    "Run K-means for values of $k$ ranging from 2 to 20 on `aggregation` dataset. Record Silhouette Coefficient, Calinski-Harabasz index, and Davies-Bouldin index for each value of $k$. Plot these measure values in relation to $k$ using line plot. Based on the plots and measure values decide what should be the best number of clusters. This decision could be done using [elbow method](https://en.wikipedia.org/wiki/Elbow_method_(clustering)) or looking for value of $k$ that maximize or minimize the measure value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    silhouette_score,\n",
    "    calinski_harabasz_score,\n",
    "    davies_bouldin_score,\n",
    ")\n",
    "\n",
    "silhouette_scores = []\n",
    "calinski_harabasz_scores = []\n",
    "davies_bouldin_scores = []\n",
    "ks = []\n",
    "\n",
    "for k in range(2, 21):\n",
    "    ks.append(k)\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    clusters = kmeans.fit_predict(aggregation_train)\n",
    "    silhouette_scores.append(silhouette_score(aggregation_train, clusters))\n",
    "    calinski_harabasz_scores.append(\n",
    "        calinski_harabasz_score(aggregation_train, clusters)\n",
    "    )\n",
    "    davies_bouldin_scores.append(davies_bouldin_score(aggregation_train, clusters))\n",
    "\n",
    "performance = pd.DataFrame(\n",
    "    {\n",
    "        \"k\": ks,\n",
    "        \"silhouette\": silhouette_scores,\n",
    "        \"calinski_harabasz\": calinski_harabasz_scores,\n",
    "        \"davies_bouldin\": davies_bouldin_scores,\n",
    "    }\n",
    ")\n",
    "\n",
    "sns.lineplot(data=performance, x=\"k\", y=\"silhouette\").set(xticks=ks)\n",
    "plt.show()\n",
    "sns.lineplot(data=performance, x=\"k\", y=\"calinski_harabasz\").set(xticks=ks)\n",
    "plt.show()\n",
    "sns.lineplot(data=performance, x=\"k\", y=\"davies_bouldin\").set(xticks=ks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density-Based Spatial Clustering of Applications with Noise (DBSCAN)\n",
    "\n",
    "DBSCAN is one of the more sophisticated clustering techniques. Its basic idea is that clusters are areas with higher density of points separated by areas of lower densities. It has not assumption on cluster shapes (K-means expects convex clusters), does not need to specify number of clusters in advance (K-means needs number of clusters, HAC can cluster based on distance in tree), and is insensitive to outliers (K-means and HAC can be affected). You can read more on differences and advantages of DBSCAN in this [blog post](https://towardsdatascience.com/dbscan-clustering-for-data-shapes-k-means-cant-handle-well-in-python-6be89af4e6ea).\n",
    "\n",
    "Let's demonstrate this advantages on another synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(random_state=170, n_samples=600, centers=5)\n",
    "np.random.seed(74)\n",
    "transformation = np.random.normal(size=(2, 2))  # transform for streching the data\n",
    "X = np.dot(X, transformation)  # transform points\n",
    "diagonal = pd.DataFrame(X, columns=[\"x\", \"y\"])\n",
    "diagonal[\"label\"] = y\n",
    "\n",
    "plot_clusters(diagonal, diagonal.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well K-Means and HAC are doing on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagonal_train = diagonal.drop(columns=\"label\")\n",
    "km = KMeans(n_clusters=5)\n",
    "km.fit(diagonal_train)\n",
    "\n",
    "plot_clusters(diagonal, km.labels_)\n",
    "plt.show()\n",
    "\n",
    "hac = AgglomerativeClustering(n_clusters=5)\n",
    "hac.fit(diagonal_train)\n",
    "plot_clusters(diagonal, hac.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that clusters in the upper left corner are incorrectly recognized in both cases. This type of clusters is tricky for HAC and K-means as 'ends' of the clusters are further apart than points between clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>Exercise 4</b></div>\n",
    "\n",
    "Find clusters in `diagonal` dataset using DBSCAN and plot them using `plot_clusters` function. The result should look like [this](https://www.fi.muni.cz/~xcechak1/IB031/assets/dbscan_diagonal.png)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN()\n",
    "dbscan.fit(diagonal_train)\n",
    "plot_clusters(diagonal, dbscan.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here DBSCAN produced noticeably better result. Few black dots are \"outliers\" that do not belong to any cluster according to DBSCAN. However, it is not always this easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>Exercise 5</b></div>\n",
    "\n",
    "Find clusters in `aggregation` dataset using DBSCAN and plot them using `plot_clusters` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN()\n",
    "clusters = dbscan.fit_predict(aggregation_train)\n",
    "\n",
    "plot_clusters(aggregation_train, clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is not quite what we want. There are clearly clusters not just bunch of outliers. Big disadvantage of DBSCAN is its sensitivity to hyper parameter setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>Exercise 6</b></div>\n",
    "\n",
    "Tweak DBSCAN's parameters so it identifies clusters correctly. Look into documentation for more info about each parameters function. Try to come close to the clustering in [this](https://www.fi.muni.cz/~xcechak1/IB031/assets/dbscan_aggregation.png)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=1.4, min_samples=7)\n",
    "clusters = dbscan.fit_predict(aggregation_train)\n",
    "plot_clusters(aggregation_train, clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Mixture\n",
    "\n",
    "Gaussian mixture model fits gaussians ([generalization of gauss curve](https://en.wikipedia.org/wiki/Gaussian_function#Two-dimensional_Gaussian_function) to higher dimensions) for each cluster. Each point is then assign to cluster whose gaussian has the highest density at that point. This is of course a limiting factor (recall bias-variance trade-off) but it allows to model clusters that are separated by another cluster like the one below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "cross = pd.DataFrame()\n",
    "x = np.random.normal(1, 1, 100)\n",
    "y = np.random.normal(0, 1, 100)\n",
    "cross = cross.append(pd.DataFrame({\"x\": x, \"y\": y, \"label\": 0}))\n",
    "\n",
    "x = np.random.normal(0, 0.1, 100)\n",
    "y = np.random.normal(0, 5, 100)\n",
    "cross = cross.append(pd.DataFrame({\"x\": x, \"y\": y, \"label\": 1}))\n",
    "\n",
    "plot_clusters(cross, cross.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>Exercise 7</b></div>\n",
    "\n",
    "Fit GaussianMixture model on `cross` dataset and plot them using `plot_clusters` function. Observe that points on both sides of the vertical cluster belong to the same cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "gmm = GaussianMixture(n_components=2)\n",
    "clusters = gmm.fit_predict(cross)\n",
    "plot_clusters(cross, clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because guassian for horizontal cluster is 'flatter' and has higher variance. So the density is still higher even for points to the left of the other cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buckshot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\"><b>Exercise 8</b></div>\n",
    "\n",
    "Implement Buckshot algorithm using AgglomerativeClustering and KMeans from `scikit-learn` and compare its performance on `aggregation` dataset to AgglomerativeClustering and KMeans. The description of the Buckshot algorithm is in the slides from lecture 4 on clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClusterMixin\n",
    "\n",
    "\n",
    "class BuckshotClustering(ClusterMixin, BaseEstimator):\n",
    "    def __init__(self, n_clusters):\n",
    "        self.n_clusters = n_clusters\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        # select sqrt(n) samples\n",
    "        n = X.shape[0]\n",
    "        sample_indices = np.random.choice(np.arange(n), int(np.sqrt(n)), replace=False)\n",
    "        sample_X = X[sample_indices]\n",
    "\n",
    "        # fit HAC\n",
    "        agg_clust = AgglomerativeClustering(n_clusters=self.n_clusters)\n",
    "        agg_clust.fit(sample_X)\n",
    "\n",
    "        # compute centroids\n",
    "        data = pd.DataFrame(sample_X)\n",
    "        data[\"label\"] = agg_clust.labels_\n",
    "        centroids = data.groupby(\"label\").mean()\n",
    "\n",
    "        # fit K means with custom centroids\n",
    "        kmean_clust = KMeans(\n",
    "            n_clusters=self.n_clusters, init=centroids.values, n_init=1\n",
    "        )\n",
    "        kmean_clust.fit(X)\n",
    "\n",
    "        # technicality of framework\n",
    "        self.labels_ = kmean_clust.labels_\n",
    "        return self\n",
    "\n",
    "\n",
    "ac = AgglomerativeClustering(n_clusters=7)\n",
    "ac.fit(aggregation_train)\n",
    "\n",
    "km = KMeans(n_clusters=7)\n",
    "km.fit(aggregation_train)\n",
    "\n",
    "bs = BuckshotClustering(n_clusters=7)\n",
    "bs.fit(aggregation_train)\n",
    "\n",
    "print(\"HAC: \", silhouette_score(aggregation_train, ac.labels_))\n",
    "print(\"k-means: \", silhouette_score(aggregation_train, km.labels_))\n",
    "print(\"buckshot: \", silhouette_score(aggregation_train, bs.labels_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ib031",
   "language": "python",
   "name": "ib031"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
