{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = sns.load_dataset(\"titanic\")\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = titanic.dropna()\n",
    "X, y = titanic[[\"sex\", \"pclass\", \"sibsp\"]].copy(), titanic.survived\n",
    "X[\"sex\"] = (X[\"sex\"] == \"female\").astype(int)\n",
    "pd.concat([X, y], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nový exmaple\n",
    "určit, jak bude klasifikovaný\n",
    "jak by musel bypadat example, aby padl to XY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aim to \"explain\" the data by subdividing the data in to smaller parts that have more homogeneous labels. Meaning this particular group defined by subdivisions, e.g., woman from 1st class, mostly survived. To do that we need some way of measuring the homogeneity of labels. This can be done using entropy that measures amount of information (or randomness) in the data. It is defined as \n",
    "\n",
    "$$ H(X) = - \\sum_{k} p_{k} log(p_{k}) $$\n",
    "\n",
    "where $X$ is the data and $p_{k}$ is the relative frequency (or probability) of label $k$ in the $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need frequency of labels,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = y.value_counts()\n",
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make them relative,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = values[0] / len(y)\n",
    "p1 = values[1] / len(y)\n",
    "p0, p1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and sum up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = -(p0 * np.log2(p0) + p1 * np.log2(p1))\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a function from this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y):\n",
    "    probabilities = y.value_counts() / len(y)\n",
    "    return -np.sum(probabilities * np.log2(probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know how to measure homogeneity of labels but we need to decide on how to split the data. One way is to measure change in the entropy before and after splitting. This is called information gain and it is defined as\n",
    "\n",
    "$$ Gain(X, F) = Entropy(X) - \\sum_{v \\in values(F)} \\frac{|X_v|}{|X|} Entropy(X_v) $$\n",
    "\n",
    "where $X$ is the data, $F$ is the feature by which value we will split the data, and $X_v$ is a subset of the data with feature $F$ having value $v$.\n",
    "\n",
    "Let's compute information gain for feature `sex` that has two possible values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sex_0 = y[X[\"sex\"] == 0]\n",
    "y_sex_1 = y[X[\"sex\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(y_sex_0), entropy(y_sex_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_sex_0) / len(y) * entropy(y_sex_0), len(y_sex_1) / len(y) * entropy(y_sex_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(y_sex_0) / len(y) * entropy(y_sex_0)) + (len(y_sex_1) / len(y) * entropy(y_sex_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(y) - (\n",
    "    (len(y_sex_0) / len(y) * entropy(y_sex_0))\n",
    "    + (len(y_sex_1) / len(y) * entropy(y_sex_1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a function from this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_gain(X, y, feature):\n",
    "    feature_values = X[feature].unique()\n",
    "    splited_y = [y[X[feature] == value] for value in feature_values]\n",
    "    return entropy(y) - sum(\n",
    "        len(subset_y) / len(y) * entropy(subset_y) for subset_y in splited_y\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_gain(X, y, \"sex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can asses how much information we gain by splitting by each feature. All that remains is to select the one that has maximal informational gain a use it as for splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_gain(X, y, \"sex\"), info_gain(X, y, \"pclass\"), info_gain(X, y, \"sibsp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the scikit-learn's implementation also select `sex` in the first split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth=1)\n",
    "dtc.fit(X, y)\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "tree.plot_tree(dtc, filled=True, rounded=True, feature_names=X.columns)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, let's continue recursively splitting leafs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sex0, y_sex0 = X[X.sex == 0], y[X.sex == 0]\n",
    "X_sex1, y_sex1 = X[X.sex == 1], y[X.sex == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_gain(X_sex0, y_sex0, \"pclass\"), info_gain(X_sex0, y_sex0, \"sibsp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_gain(X_sex1, y_sex1, \"pclass\"), info_gain(X_sex1, y_sex1, \"sibsp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose `sbisp` to split one leaf and `pclass` to split the other. But these features are not binary, i.e. they have more than two values. We could either add multiple new nodes one for each feature value or make a split in form of values $\\leq$ threshold and values $>$ threshold. The latter is what scikit-learn's implementation is doing, so we need to find the split thresholds. Basically, we need to check every possible split between minimal and maximal feature value and check its information gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sibsp_values = sorted(X_sex0.sibsp.unique())\n",
    "pclass_values = sorted(X_sex1.pclass.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sibsp\")\n",
    "for i in range(len(sibsp_values) - 1):\n",
    "    threshold = (sibsp_values[i] + sibsp_values[i + 1]) / 2\n",
    "    y_leq = y_sex0[X_sex0.sibsp <= threshold]\n",
    "    y_ge = y_sex0[X_sex0.sibsp > threshold]\n",
    "    gain = entropy(y_sex0) - (\n",
    "        len(y_leq) / len(y_sex0) * entropy(y_leq)\n",
    "        + len(y_ge) / len(y_sex0) * entropy(y_ge)\n",
    "    )\n",
    "    print(f\"threshold: {threshold}, info gain: {gain}\")\n",
    "\n",
    "print(\"pclass\")\n",
    "for i in range(len(pclass_values) - 1):\n",
    "    threshold = (pclass_values[i] + pclass_values[i + 1]) / 2\n",
    "    y_leq = y_sex1[X_sex1.pclass <= threshold]\n",
    "    y_ge = y_sex1[X_sex1.pclass > threshold]\n",
    "    gain = entropy(y_sex1) - (\n",
    "        len(y_leq) / len(y_sex1) * entropy(y_leq)\n",
    "        + len(y_ge) / len(y_sex1) * entropy(y_ge)\n",
    "    )\n",
    "    print(f\"threshold: {threshold}, info gain: {gain}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we split on `sibsp` at $0.5$ and on `pclass` on $2.5$. Let's check with scikit-learn's implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth=2)\n",
    "dtc.fit(X, y)\n",
    "\n",
    "plt.figure(dpi=200)\n",
    "tree.plot_tree(\n",
    "    dtc,\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    node_ids=True,\n",
    "    feature_names=X.columns,\n",
    "    class_names=(\"Died\", \"Survived\"),\n",
    ")\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying new data\n",
    "Let's say we have following new data to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X = pd.DataFrame({\"sex\": [0, 1, 1], \"pclass\": [1, 2, 3], \"sibsp\": [0, 1, 3],})\n",
    "new_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who will these examples be classified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc.predict(new_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc.decision_path(new_X).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final notes\n",
    "\n",
    "### Impurity measure\n",
    "By default, the label impurity measure in scikit-learn's implementation is Gini index and not entropy. They are fairly similar and Gini is defined as\n",
    "\n",
    "$$ H(X) = \\sum_{k} p_{k}(1-p_{k}) $$\n",
    "\n",
    "where, again, $X$ is the data and $p_k$ is relative proportion of label $k$. The main advantage of Gini over entropy is not biased towards features with many distinct values.\n",
    "\n",
    "### Pruning\n",
    "The presented tree building schema is pretty much ID3 algorithm. It is known to over-fit the data a lot. Most library implementations of decision trees are either C4.5 or CART algorithms that are based on ID3 and add few additional ideas. One of them is pruning. Pruning results in some nodes not being split if they do not significantly increase precision (pre-pruning) and some node being collapsed back into one node if the prediction accuracy is not affected much (post-pruning)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
